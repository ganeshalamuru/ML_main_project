{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkfbOU2_3SoU"
   },
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzpgOaAAfq2Q"
   },
   "outputs": [],
   "source": [
    "!rm -rf ML_main_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "2jP_UCitiwF6",
    "outputId": "a889362f-ddfa-4544-e641-34f116ace3de"
   },
   "outputs": [],
   "source": [
    "#Getting the task files and gym from git\n",
    "!git clone https://github.com/ganeshalamuru/ML_main_project.git ML_main_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dSaQleffbyIB",
    "outputId": "14754676-175b-4af9-d9bb-78eeb9fa6f26"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from random import randint\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import keras\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "from statistics import median, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "YOaGwTkcbcEN",
    "outputId": "b8a59da7-6903-47e5-9458-3d8fece82053"
   },
   "outputs": [],
   "source": [
    "# Render the start state of the cartpole game\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VF8UewD3ddK"
   },
   "source": [
    "# Naive Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zlwwq-2pxDCa"
   },
   "outputs": [],
   "source": [
    "max_episode_length = 500\n",
    "num_test_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJQddCLLh4_0"
   },
   "outputs": [],
   "source": [
    "# Naive Q Learning\n",
    "# Q Learning requires discrete observation spaces. \n",
    "# Obsevation space of cartpole is continuous and must be first made discrete.\n",
    "class CartPole():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=100, num_test_episodes = 100, min_alpha=0.1, min_epsilon=0.1, gamma=0.9, adaptive_rate_divisor=25, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.num_test_episodes = num_test_episodes # test episodes\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.adaptive_rate_divisor = adaptive_rate_divisor # Divide by this constant for the adaptive learning,exploration rates\n",
    "\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        # initialising Q-table\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    # Discretizing input space to make Q-table\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    # Choosing action based on epsilon-greedy policy: Choose a random action if random number generated <= epsilon\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    # Updating Q-value of state-action pair based on the update equation\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    # Adaptive learning of Exploration Rate where t is episode number\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.adaptive_rate_divisor)))\n",
    "\n",
    "    # Adaptive learning of Learning Rate whre t is episode number\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.adaptive_rate_divisor)))\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scores = []\n",
    "        for e in range(self.n_episodes):\n",
    "            # As states are continuous, discretize them into buckets\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            # Get adaptive learning alpha and epsilon decayed over time\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                # Render environment\n",
    "                #self.env.render()\n",
    "\n",
    "                # Choose action according to greedy policy and take it\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "\n",
    "                # Update Q-Table\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1 \n",
    "            scores.append(i)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Training Episodes')\n",
    "        plt.show()\n",
    "        \n",
    "      \n",
    "    def evaluate(self):\n",
    "\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        scores = []\n",
    "        for e in range(self.num_test_episodes):\n",
    "            # As states are continuous, discretize them into buckets\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                # Render environment\n",
    "                #self.env.render()\n",
    "\n",
    "                # Choose action and take it\n",
    "                action = self.choose_action(current_state,0)\n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "            scores.append(i)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Test Episodes')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "2NsFQY5fMIov",
    "outputId": "2f1446a7-ef80-4363-b3c0-32061ed4b09f"
   },
   "outputs": [],
   "source": [
    "Q_Learning_Model = CartPole(n_episodes=100)\n",
    "Q_Learning_Model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "N00GvqrCUKdI",
    "outputId": "f53a86d8-881a-49b5-de09-21c24ce12c5a"
   },
   "outputs": [],
   "source": [
    "Q_Learning_Model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew2mxv3Sad-H"
   },
   "outputs": [],
   "source": [
    "with open('Q-Learning-Model-Best.pkl', 'wb') as output:\n",
    "    pickle.dump(Q_Learning_Model, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asSScE0NupTc"
   },
   "source": [
    "# Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqbNEp0zZgo6"
   },
   "outputs": [],
   "source": [
    "# Deep Q Learning\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "EPSILON = 0.9\n",
    "DECAY = 0.995\n",
    "GAMMA = 0.95\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "REPLAY_BUFFER_CAPACITY = 2000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.n\n",
    "N_TRAIN_EPISODES = 1000\n",
    "N_TEST_EPISODES = 100\n",
    "\n",
    "# Initialise weights from normal distribution\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.1)\n",
    "\n",
    "# Store the experiences in the replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, x):\n",
    "        self.memory.append(x)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def get_len(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Neural Network Definition\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(STATE_DIM, 50)\n",
    "        self.fc2 = nn.Linear(50, ACTION_DIM)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "# Forward Propagation of an Input\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Agent who plays the game\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.dqn, self.target_dqn = DQN(), DQN()\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_counter = 0\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "        self.optimizer = opt.Adam(self.dqn.parameters(), lr=LR)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Choosing action based on epsilon-greedy policy: Choose a random action if random number generated <= epsilon\n",
    "    def choose_action(self, s,epsilon):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0)\n",
    "\n",
    "        if np.random.uniform() > epsilon:\n",
    "            qs = self.dqn.forward(s)\n",
    "            action = torch.max(qs, 1)[1].data.numpy()\n",
    "            action = action[0]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Update parameters of the NN\n",
    "    def update_params(self):\n",
    "        # update target network\n",
    "        if self.learn_step_counter % TARGET_UPDATE_INTERVAL == 0:\n",
    "            self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch of transitions\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(\n",
    "            BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions.astype(int).reshape((-1, 1)))\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(np.float32(dones)).unsqueeze(1)\n",
    "\n",
    "        # get q values\n",
    "        q_current = self.dqn(states).gather(1, actions)\n",
    "        q_next = self.target_dqn(next_states).detach()\n",
    "        q_target = rewards + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        q_loss = self.loss_fn(q_current, q_target)\n",
    "\n",
    "        # backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scores = []\n",
    "        for i in range(N_TRAIN_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "            self.epsilon = EPSILON\n",
    "            while True:\n",
    "              # env.render()\n",
    "              action = self.choose_action(state,self.epsilon)\n",
    "              self.epsilon *= DECAY\n",
    "              self.epsilon = max(0.1,self.epsilon)\n",
    "\n",
    "              # take action\n",
    "              next_state, reward_orig, done, _ = env.step(action)\n",
    "              step += 1\n",
    "\n",
    "              # modify the reward function\n",
    "              x, x_dot, theta, theta_dot = next_state\n",
    "              r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "              r2 = (\n",
    "              env.theta_threshold_radians - abs(theta)\n",
    "              ) / env.theta_threshold_radians - 0.5\n",
    "              reward = r1 + r2\n",
    "\n",
    "              self.replay_buffer.push((state, action, reward, next_state, done))\n",
    "              self.memory_counter += 1\n",
    "\n",
    "              episode_reward += reward_orig\n",
    "\n",
    "              if self.memory_counter > REPLAY_BUFFER_CAPACITY:\n",
    "                self.update_params()\n",
    "\n",
    "              if done:\n",
    "                print(\n",
    "                    \"Episode: {}, Reward: {}, step: {}\".format(\n",
    "                        i, round(episode_reward, 2), step\n",
    "                    )\n",
    "                )\n",
    "\n",
    "              if done:\n",
    "                break\n",
    "\n",
    "              state = next_state\n",
    "            scores.append(step)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Train Episodes')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        scores = []\n",
    "        for i in range(N_TEST_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "\n",
    "            while True:\n",
    "              # env.render()\n",
    "              action = self.choose_action(state,0)\n",
    "\n",
    "              # take action\n",
    "              next_state, reward_orig, done, _ = env.step(action)\n",
    "              step += 1\n",
    "              state = next_state\n",
    "              if done:\n",
    "                break\n",
    "\n",
    "            scores.append(step)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Test Episodes')\n",
    "        plt.show()      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0e5zSxskRvbP",
    "outputId": "2d1564a5-fb97-46c7-bc26-c79322897b8a"
   },
   "outputs": [],
   "source": [
    "Deep_Q_Learning_Model = Agent()\n",
    "Deep_Q_Learning_Model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "LC5UZuKOTb7d",
    "outputId": "d71ba7b8-891a-4d95-8112-e1626cd547e5"
   },
   "outputs": [],
   "source": [
    "Deep_Q_Learning_Model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok7whL0QowZ9"
   },
   "outputs": [],
   "source": [
    "with open('Deep-Q-Learning-Model-Best-Extra.pkl', 'wb') as output:\n",
    "    pickle.dump(Deep_Q_Learning_Model, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "244RwrLIbVgj"
   },
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HDIAUtkCeti"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "                \n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "    def evaluate(self,num_episodes=100):\n",
    "        env_name = \"CartPole-v1\"\n",
    "        env = gym.make(env_name)\n",
    "        memory = Memory()\n",
    "        all_episode_rewards = []\n",
    "        x_axis = []\n",
    "        y_axis = []\n",
    "        for i in range(num_episodes):\n",
    "            ep_reward = 0\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = self.policy_old.act(state, memory)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                ep_reward+=reward\n",
    "            all_episode_rewards.append(ep_reward)\n",
    "            x_axis.append(i)\n",
    "            y_axis.append(ep_reward)\n",
    "        mean_episode_reward = np.mean(all_episode_rewards)\n",
    "        print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
    "        plt.plot(x_axis,y_axis)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Test Episodes')\n",
    "        plt.show()\n",
    "        return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "ed3IySDu6g2S",
    "outputId": "5d0716da-be67-4d09-aad9-6df109874006"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    ############## Hyperparameters ##############\n",
    "    env_name = \"CartPole-v1\"\n",
    "    # creating environment\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = 2\n",
    "    render = False\n",
    "    solved_reward = 500         # stop training if avg_reward > solved_reward\n",
    "    log_interval = 100          # print avg reward in the interval\n",
    "    max_episodes = 3500        # max training episodes\n",
    "    max_timesteps = 500         # max timesteps in one episode\n",
    "    n_latent_var = 64           # number of variables in hidden layer\n",
    "    update_timestep = 700      # update policy every n timesteps\n",
    "    lr = 0.002\n",
    "    betas = (0.9, 0.999)\n",
    "    gamma = 0.99               # discount factor\n",
    "    K_epochs = 8                # update policy for K epochs\n",
    "    eps_clip = 0.2              # clip parameter for PPO\n",
    "    #############################################\n",
    "    \n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "    print(lr,betas)\n",
    "    \n",
    "    # logging variables\n",
    "    running_reward = 0\n",
    "    avg_length = 0\n",
    "    timestep = 0\n",
    "    x_axis = []\n",
    "    y_axis = []\n",
    "    # training loop\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "            \n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Saving reward and is_terminal:\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                timestep = 0\n",
    "            \n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        avg_length += t\n",
    "        # logging\n",
    "        if i_episode % log_interval == 0:\n",
    "            avg_length = int(avg_length/log_interval)\n",
    "            running_reward = int((running_reward/log_interval))\n",
    "            x_axis.append(i_episode/log_interval)\n",
    "            y_axis.append(running_reward)  \n",
    "            print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "            running_reward = 0\n",
    "            avg_length = 0\n",
    "    with open('./PPO_CartPole-v1.pkl', 'wb') as output:\n",
    "        pickle.dump(ppo, output, pickle.HIGHEST_PROTOCOL)    \n",
    "    # Print some stats\n",
    "    plt.plot(x_axis,y_axis)\n",
    "    plt.ylabel('Scores(averaged over 100)')\n",
    "    plt.xlabel('Train Episodes(per 100)')\n",
    "    plt.show()   \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_5E93WTMrsB"
   },
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_path = gym.__file__\n",
    "gym_path = gym_path.replace('/__init__.py','')\n",
    "print(gym_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model obtained from task2.py seems to be the best (for all three tasks)\n",
    "subprocess.call('cp ./ML_main_project/task2.py {}/envs/classic_control/cartpole.py'.format(gym_path), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mrxjqBWVg-b"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "ind = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoZqi7L0Vjk5"
   },
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "test_runs = 10\n",
    "num_generations = 1500\n",
    "\n",
    "# For evaluation\n",
    "episode_length = 500\n",
    "num_test_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BYJA9v9VmgC"
   },
   "outputs": [],
   "source": [
    "# Custom Neural Network\n",
    "class NN():\n",
    "    act_space_n = act_dim\n",
    "    def __init__(self,obs,in_w,in_b,hid_w,out_w):\n",
    "        super().__init__()\n",
    "        self.action = self.calculate_act(obs,in_w,in_b,hid_w,out_w)\n",
    "\n",
    "    def calculate_act(self,obs,in_w,in_b,hid_w,out_w):\n",
    "        obs = obs/max(np.max(np.linalg.norm(obs)),1)\n",
    "        in_layer = self.reLu(np.dot(obs,in_w)+in_b.T)\n",
    "        hid_layer_1 = self.reLu(np.dot(in_layer,hid_w))\n",
    "        hid_layer_2 = np.dot(hid_layer_1,out_w)\n",
    "        output = self.reLu(hid_layer_2)\n",
    "        output = self.softmax(output)\n",
    "        output = output.argsort().reshape(1,NN.act_space_n)\n",
    "        return output[0][0] # discrete action\n",
    "\n",
    "    def reLu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x = np.exp(x)/np.sum(np.exp(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-nuFp9JVoRt"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "in_node_num = 4\n",
    "hid_node_num = 2\n",
    "mutation_power = 10\n",
    "crossover_generations = 10\n",
    "\n",
    "class Agent():\n",
    "\tact_space_n = act_dim\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\tdef mutate(self,new_dna):\n",
    "\t\tj = np.random.randint(0,len(new_dna))\n",
    "\t\tif ( 0 < j < mutation_power): # controlling rate of mutation\n",
    "\t\t\tfor i in range(j):\n",
    "\t\t\t\tn = np.random.randint(0,len(new_dna)) #random position for mutation\n",
    "\t\t\t\tnew_dna[n] = new_dna[n] + np.random.rand()\n",
    "\n",
    "\t\tmut_dna = new_dna\n",
    "\t\treturn mut_dna\n",
    "\n",
    "\tdef crossover(self,Dna_list):\n",
    "\t\tnewDNA_list = []\n",
    "\t\tnewDNA_list.append(Dna_list[0])\n",
    "\t\tnewDNA_list.append(Dna_list[1]) \n",
    "\n",
    "\t\tfor l in range(crossover_generations):  # number of generations after crossover\n",
    "\t\t\tj = np.random.randint(0,len(Dna_list[0]))\n",
    "\t\t\tnew_dna = np.append(Dna_list[0][:j], Dna_list[1][j:])\n",
    "\n",
    "\t\t\tmut_dna = self.mutate(new_dna)\n",
    "\t\t\tnewDNA_list.append(mut_dna)\n",
    "\n",
    "\t\treturn newDNA_list\n",
    "\n",
    "\tdef intial_gen(self):\n",
    "\n",
    "\t\tin_w = []\n",
    "\t\tin_b = []\n",
    "\n",
    "\t\thid_w = []\n",
    "\t\tout_w = [] \n",
    "\n",
    "\t\tin_node = in_node_num\n",
    "\t\thid_node = hid_node_num\n",
    "\n",
    "\t\tfor i in range(self.test_runs):\n",
    "\n",
    "\t\t\tin_w.append(np.random.rand(ind,in_node))\n",
    "\n",
    "\t\t\tin_b.append(np.random.rand((in_node)))\n",
    "\n",
    "\t\t\thid_w.append(np.random.rand(in_node,hid_node))\n",
    "\n",
    "\t\t\tout_w.append(np.random.rand(hid_node, Agent.act_space_n))\n",
    "\n",
    "\t\treturn [in_w, in_b, hid_w, out_w] # This is the generation\n",
    "\n",
    "\n",
    "\tdef run_env(self,in_w,in_b,hid_w,out_w):\n",
    "\t\tobs = self.env.reset()\n",
    "\t\taward = 0\n",
    "\t\tfor t in range(episode_length):\n",
    "\t\t\taction = NN(obs,in_w,in_b,hid_w,out_w).action\n",
    "\t\t\tobs, reward, done, _ = self.env.step(action)\n",
    "\t\t\taward += reward \n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\treturn award       \n",
    "\n",
    "\tdef rand_run(self):\n",
    "\t\taward_set = []\n",
    "\t\tgenerations = self.intial_gen()\n",
    "\t\tfor episode in range(self.test_runs):\n",
    "\t\t\tin_w  = generations[0][episode]\n",
    "\t\t\tin_b = generations[1][episode]\n",
    "\t\t\thid_w =  generations[2][episode]\n",
    "\t\t\tout_w =  generations[3][episode]\n",
    "\t\t\taward = self.run_env(in_w,in_b,hid_w,out_w)\n",
    "\t\t\taward_set = np.append(award_set,award)\n",
    "\n",
    "\t\tgen_award = [generations, award_set]\n",
    "\t\treturn gen_award\n",
    "\n",
    "\tdef evolve(self,award_set, generations):\n",
    "\n",
    "\t\tgood_award_idx = award_set.argsort()[-2:][::-1] # best 2 are selected \n",
    "\t\tgood_generation = []\n",
    "\t\tDNA_list = []\n",
    "\n",
    "\t\tnew_input_weight = []\n",
    "\t\tnew_input_bias = []\n",
    "\n",
    "\t\tnew_hidden_weight = []\n",
    "\n",
    "\t\tnew_output_weight =[]\n",
    "\n",
    "\t\tnew_award_set = []\n",
    "\n",
    "\n",
    "\t\t#Extraction of all weight info into a single sequence\n",
    "\t\tfor index in good_award_idx:\n",
    "\t\t\t\n",
    "\t\t\tw1 = generations[0][index]\n",
    "\t\t\tdna_in_w = w1.reshape(w1.shape[1],-1)\n",
    "\n",
    "\t\t\tb1 = generations[1][index]\n",
    "\t\t\tdna_b1 = np.append(dna_in_w, b1)\n",
    "\t\t\tw2 = generations[2][index]\n",
    "\t\t\tdna_whid = w2.reshape(w2.shape[1],-1)\n",
    "\t\t\tdna_w2 = np.append(dna_b1,dna_whid)\n",
    "\t\t\t\n",
    "\t\t\twh = generations[3][index]\n",
    "\t\t\tdna = np.append(dna_w2, wh)\n",
    "\n",
    "\t\t\tDNA_list.append(dna) # take 2 dna for good generation\n",
    "\n",
    "\t\tnewDNA_list = self.crossover(DNA_list)\n",
    "\n",
    "\t\tfor newdna in newDNA_list: # collection of weights from dna info\n",
    "\t\t\t\n",
    "\t\t\tnewdna_in_w1 = np.array(newdna[:generations[0][0].size]) \n",
    "\t\t\tnew_in_w = np.reshape(newdna_in_w1, (-1,generations[0][0].shape[1]))\n",
    "\t\t\tnew_input_weight.append(new_in_w)\n",
    "\n",
    "\t\t\tnew_in_b = np.array([newdna[newdna_in_w1.size:newdna_in_w1.size+generations[1][0].size]]).T #bias\n",
    "\t\t\tnew_input_bias.append(new_in_b)\n",
    "\n",
    "\t\t\tsh = newdna_in_w1.size + new_in_b.size\n",
    "\t\t\tnewdna_in_w2 = np.array([newdna[sh:sh+generations[2][0].size]])\n",
    "\t\t\tnew_hid_w = np.reshape(newdna_in_w2, (-1,generations[2][0].shape[1]))\n",
    "\t\t\tnew_hidden_weight.append(new_hid_w)\n",
    "\n",
    "\t\t\tsl = newdna_in_w1.size + new_in_b.size + newdna_in_w2.size\n",
    "\t\t\tnew_out_w = np.array([newdna[sl:]]).T\n",
    "\t\t\tnew_out_w = np.reshape(new_out_w, (-1,generations[3][0].shape[1]))\n",
    "\t\t\tnew_output_weight.append(new_out_w)\n",
    "\n",
    "\t\t\tnew_award = self.run_env(new_in_w, new_in_b, new_hid_w, new_out_w) #bias\n",
    "\t\t\tnew_award_set = np.append(new_award_set,new_award)\n",
    "\n",
    "\t\tnew_generation = [new_input_weight,new_input_bias,new_hidden_weight,new_output_weight]\n",
    "\t\treturn new_generation, new_award_set\n",
    "\n",
    "\tdef train(self,test_runs,num_generations):\n",
    "\t\tself.test_runs = test_runs\n",
    "\t\tself.num_generations = num_generations\n",
    "\t\tgen_award = self.rand_run()\n",
    "\t\tcurrent_gens = gen_award[0]\n",
    "\t\tcurrent_award_set = gen_award[1]\n",
    "\t\tbest_gen =[]\n",
    "\t\tA =[]\n",
    "\t\tmax_avg = 0\n",
    "\t\tfor n in range(num_generations):\n",
    "\t\t\tnew_generation, new_award_set = self.evolve(current_award_set, current_gens)\n",
    "\t\t\tcurrent_gens = new_generation\n",
    "\t\t\tcurrent_award_set = new_award_set\n",
    "\t\t\tavg = np.average(current_award_set)\n",
    "\t\t\ta = np.amax(current_award_set)\n",
    "\t\t\tif avg == episode_length:\n",
    "\t\t\t\tbest_gen = np.array([current_gens[0][0],current_gens[1][0],current_gens[2][0],current_gens[3][0]])\n",
    "\t\t\t\tself.best_gen = best_gen\n",
    "\t\t\t\t\n",
    "\t\t\tprint(\"generation: {}, average: {}\".format(n+1, avg))\n",
    "\t\t\tA = np.append(A, avg)\n",
    "\t\t\t\n",
    "\t\tplt.plot(A)\n",
    "\t\tplt.xlabel('Generations')\n",
    "\t\tplt.ylabel('Avg Score')\n",
    "\t\tplt.grid()\n",
    "\n",
    "\t\tprint('Average score:',mean(A))\n",
    "\t\tprint('Median score:',median(A))\n",
    "\t\treturn plt.show()\n",
    "\n",
    "\tdef test_run_env(self,in_w,in_b,hid_w,out_w):\n",
    "\t\tobs = self.env.reset()\n",
    "\t\taward = 0\n",
    "\t\tfor t in range(episode_length):\n",
    "\t\t\taction = NN(obs,in_w,in_b,hid_w,out_w).action\n",
    "\t\t\tobs, reward, done, info = self.env.step(action)\n",
    "\t\t\taward += reward\n",
    "\t\t\t\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\tself.env.close()\n",
    "\t\treturn award\n",
    "\n",
    "\tdef test_run_n_times(self,in_w, in_b, hid_w,out_w,n_times):\n",
    "\t\taverage = []\n",
    "\t\tfor i in range(n_times):\n",
    "\t\t\taverage.append(self.test_run_env(in_w,in_b,hid_w,out_w))\n",
    "\t\t\t\n",
    "\t\tplt.plot(average)\n",
    "\t\tplt.xlabel('Episodes')\n",
    "\t\tplt.ylabel('Score')\n",
    "\t\tplt.grid()\n",
    "\t\treturn mean(average)\n",
    "\n",
    "\tdef evaluate(self,n_times):\n",
    "\t\tparam = self.best_gen\n",
    "\t\tself.env = gym.make('CartPole-v1')\n",
    "\t\tin_w  = param[0]\n",
    "\t\tin_b  = param[1]\n",
    "\t\thid_w = param[2]\n",
    "\t\tout_w = param[3]\n",
    "\t\treturn self.test_run_n_times(in_w, in_b, hid_w,out_w,n_times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hw_TIzT6VqfG",
    "outputId": "896ba202-20b8-4c5c-ae87-4c5adddb96fd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Note: Sometimes the random seed can cause the model to fail after a few generations..Please rerun again then.\n",
    "GA_model = Agent()\n",
    "GA_model.train(test_runs,num_generations)\n",
    "with open('GA-Model-Best.pkl', 'wb') as output:\n",
    "    pickle.dump(GA_model, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task-A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
