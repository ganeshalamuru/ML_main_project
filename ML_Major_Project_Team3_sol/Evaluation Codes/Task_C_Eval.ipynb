{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ML_main_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SlAcp0q7JuMA",
    "outputId": "54e6fe96-50c7-4df1-c4bb-c273207fbb2a"
   },
   "outputs": [],
   "source": [
    "#Getting the task files and gym from git\n",
    "!git clone https://github.com/ganeshalamuru/ML_main_project.git ML_main_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZutaityotjLA"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7CnOfcgtvMk"
   },
   "outputs": [],
   "source": [
    "gym_path = gym.__file__\n",
    "gym_path = gym_path.replace('/__init__.py','')\n",
    "print(gym_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LEQupEw-57Zh",
    "outputId": "9d44d3d9-af39-4497-ee2a-f85a2d45f00c"
   },
   "outputs": [],
   "source": [
    "subprocess.call('cp ./ML_main_project/task3.py {}/envs/classic_control/cartpole.py'.format(gym_path), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "icEDlRb7KW8J",
    "outputId": "c07335d9-65ac-4cb8-f8eb-b16e43de80d2"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import keras\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "from statistics import median, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "colab_type": "code",
    "id": "TiS-STVHKj2l",
    "outputId": "d7ccecc5-9571-4757-94d9-42c1639d757e"
   },
   "outputs": [],
   "source": [
    "# Render the start state of the cartpole game\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Mass of the pole:\",env.masspole)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "AAP6fFQ1K0Nn",
    "outputId": "a4d6ff69-a241-43bf-8f46-10a06a28c946"
   },
   "outputs": [],
   "source": [
    "# Deep Q Learning\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "EPSILON = 0.9\n",
    "DECAY = 0.995\n",
    "GAMMA = 0.95\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "REPLAY_BUFFER_CAPACITY = 2000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.n\n",
    "N_TRAIN_EPISODES = 1000\n",
    "N_TEST_EPISODES = 100\n",
    "\n",
    "# Initialise weights from normal distribution\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.1)\n",
    "\n",
    "# Store the experiences in the replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, x):\n",
    "        self.memory.append(x)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def get_len(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Neural Network Definition\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(STATE_DIM, 50)\n",
    "        self.fc2 = nn.Linear(50, ACTION_DIM)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "# Forward Propagation of an Input\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Agent who plays the game\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.dqn, self.target_dqn = DQN(), DQN()\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_counter = 0\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "        self.optimizer = opt.Adam(self.dqn.parameters(), lr=LR)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Choosing action based on epsilon-greedy policy: Choose a random action if random number generated <= epsilon\n",
    "    def choose_action(self, s,epsilon):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0)\n",
    "\n",
    "        if np.random.uniform() > epsilon:\n",
    "            qs = self.dqn.forward(s)\n",
    "            action = torch.max(qs, 1)[1].data.numpy()\n",
    "            action = action[0]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Update parameters of the NN\n",
    "    def update_params(self):\n",
    "        # update target network\n",
    "        if self.learn_step_counter % TARGET_UPDATE_INTERVAL == 0:\n",
    "            self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch of transitions\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(\n",
    "            BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions.astype(int).reshape((-1, 1)))\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(np.float32(dones)).unsqueeze(1)\n",
    "\n",
    "        # get q values\n",
    "        q_current = self.dqn(states).gather(1, actions)\n",
    "        q_next = self.target_dqn(next_states).detach()\n",
    "        q_target = rewards + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        q_loss = self.loss_fn(q_current, q_target)\n",
    "\n",
    "        # backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        scores = []\n",
    "        for i in range(N_TRAIN_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "            self.epsilon = EPSILON\n",
    "            while True:\n",
    "              # env.render()\n",
    "              action = self.choose_action(state,self.epsilon)\n",
    "              self.epsilon *= DECAY\n",
    "              self.epsilon = max(0.1,self.epsilon)\n",
    "\n",
    "              # take action\n",
    "              next_state, reward_orig, done, _ = env.step(action)\n",
    "              step += 1\n",
    "\n",
    "              # modify the reward function\n",
    "              x, x_dot, theta, theta_dot = next_state\n",
    "              r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "              r2 = (\n",
    "              env.theta_threshold_radians - abs(theta)\n",
    "              ) / env.theta_threshold_radians - 0.5\n",
    "              reward = r1 + r2\n",
    "\n",
    "              self.replay_buffer.push((state, action, reward, next_state, done))\n",
    "              self.memory_counter += 1\n",
    "\n",
    "              episode_reward += reward_orig\n",
    "\n",
    "              if self.memory_counter > REPLAY_BUFFER_CAPACITY:\n",
    "                self.update_params()\n",
    "\n",
    "              if done:\n",
    "                print(\n",
    "                    \"Episode: {}, Reward: {}, step: {}\".format(\n",
    "                        i, round(episode_reward, 2), step\n",
    "                    )\n",
    "                )\n",
    "\n",
    "              if done:\n",
    "                break\n",
    "\n",
    "              state = next_state\n",
    "            scores.append(step)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Train Episodes')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        scores = []\n",
    "        for i in range(N_TEST_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            step = 0\n",
    "\n",
    "            while True:\n",
    "              # env.render()\n",
    "              action = self.choose_action(state,0)\n",
    "\n",
    "              # take action\n",
    "              next_state, reward_orig, done, _ = env.step(action)\n",
    "              step += 1\n",
    "              state = next_state\n",
    "              if done:\n",
    "                break\n",
    "\n",
    "            scores.append(step)\n",
    "        # Print some stats\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        plt.plot(scores)\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Test Episodes')\n",
    "        plt.show()      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "BG6ojaUfK_q8",
    "outputId": "901f87ec-857e-4145-d76f-6a179d8ed283"
   },
   "outputs": [],
   "source": [
    "with open('../Trained Models/Deep-Q-Learning-Model-Best-Extra.pkl', 'rb') as input:\n",
    "    model = pickle.load(input)\n",
    "    model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "CmdC5p7nLG1Z",
    "outputId": "71a4a864-2076-4cb1-802a-4efeed7eae12"
   },
   "outputs": [],
   "source": [
    "# Evaluate model over 100 episodes with rendering\n",
    "scores = []\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "for episode in range(0,100):\n",
    "  env.reset()\n",
    "  env.seed(42)\n",
    "  current_state = env.reset()\n",
    "  score = 0\n",
    "  for _ in range(0,500):\n",
    "    env.render()\n",
    "    action = model.choose_action(current_state,0)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    current_state = obs\n",
    "    score += 1\n",
    "    if done:\n",
    "      break\n",
    "  scores.append(score)\n",
    "# Print some stats\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('Scores:',scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g41NVe2DL8AL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task_A_Eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
